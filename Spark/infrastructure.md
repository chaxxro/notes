# 基础架构

Spark 应用程序由一个驱动器进程和一组执行器进程组成

驱动进程运行 `main` 函数，是 Spark 应用程序的核心，负责：

- 维护 Spark 应用程序的相关信息

- 回应用户的程序或输入

- 分析任务并分发给若干执行器进行处理

执行器负责执行驱动器分配的计算工作，负责：

- 执行由驱动器分配的代码

- 将执行器的计算状态报告给驱动器

## SparkSession

每一个 Spark 程序都需要一个 `SparkSession` 来作为驱动器

通过创建 `SparkSession` 来将用户命令和数据发送给 Spark

## DataFrame

`DataFrame` 是最常见的结构化 API，是包含行和列的数据表

描述这些行和列的规则成为 `schema`

`DataFrame` 是分布式的，存在于集群中

## 核心抽象

`Dataset`、`DataFrame`、SQL 表和 `RDD` 都是分布式数据集合

## 数据分区

为了让多个执行器并行处理数据，Spark 将数据分解成多个数据块，每个数据块是一个分区

分区位于集群中的一台物理机上

## 转换操作

Spark 的核心数据结构在计算过程中是保持不变的

但可以为指定抽象的转换，转换操作是使用 Spark 表达业务逻辑的核心，在调用动作操作之前，Spark 不会真的执行转换操作

有两类转换操作：

- 窄依赖关系的转换操作：每个输入分区仅决定一个输出分区的转换，输入输出是一对一的映射关系

- 宽依赖关系的转换操作 shuffle：每个输入分区决定了多个输出分区，它会在整个集群中执行互相交互分区数据，输入输出是一对多的映射关系

如果是窄依赖转换，Spark 会自动执行流水线处理，全部操作在内存中执行

如果是宽依赖转换，Spark 会将结果写入磁盘

## 惰性评估

当用户表达了一些对数据的操作时，Spark 不会立即修改数据，而是建立一个作用到原始数据的转换计划

Spark 首先将这个计划编译为可以在集群中高效运行的流水线式的物理执行计划，等到最后时刻才开始执行代码

## 动作操作

转换操作建立逻辑转换的计划，动作操作执行转换计划得到计算结果